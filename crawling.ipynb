{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make simple web crawler\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool # Pool import하기# selenium crawler\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# dir = Path(r\"C:\\Users\\wonhyeong\\workings\\data\\10X\\cleaned\") # on office\n",
    "dir = Path(r\"/Users/jowonhyeong/Desktop/workspace/data\") # on office\n",
    "index_dir = dir / 'index.pkl'\n",
    "index: pd.DataFrame = pd.read_pickle(index_dir)\n",
    "cik_list = index['cik'].unique()\n",
    "cik_list = list(map(str, cik_list))\n",
    "url_cast = 'https://sec.report/CIK/'\n",
    "url_list = list(map(lambda x: ''.join([url_cast, x]), cik_list))\n",
    "ninety = index.query('name == \"-99\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(df):\n",
    "    url_cast = 'https://www.sec.gov/Archives/edgar/data'\n",
    "    url_list = []\n",
    "    for row in df.itertuples():\n",
    "        cik = row.cik\n",
    "        acc = row.acc\n",
    "        mid_acc = acc.replace('-', '')\n",
    "        end_acc = '-'.join([acc, 'index.html'])\n",
    "        url = '/'.join([url_cast, str(cik), mid_acc, end_acc])\n",
    "        url_list.append((acc, url))\n",
    "    return url_list\n",
    "\n",
    "url_list = get_url(ninety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Service /Users/jowonhyeong/Desktop/workspace/data/chromedriver unexpectedly exited. Status code was: -9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/GoogleDrive/내 드라이브/workspace/jpynb/edgar/crawling.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39muser-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39mlang=ko_KR\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# 한국어!\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/GoogleDrive/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome(\u001b[39mdir\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mchromedriver\u001b[39;49m\u001b[39m'\u001b[39;49m, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/GoogleDrive/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GoogleDrive/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m(row):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/inbox/lib/python3.9/site-packages/selenium/webdriver/chrome/webdriver.py:73\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     66\u001b[0m         desired_capabilities\u001b[39m.\u001b[39mupdate(options\u001b[39m.\u001b[39mto_capabilities())\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice \u001b[39m=\u001b[39m Service(\n\u001b[1;32m     69\u001b[0m     executable_path,\n\u001b[1;32m     70\u001b[0m     port\u001b[39m=\u001b[39mport,\n\u001b[1;32m     71\u001b[0m     service_args\u001b[39m=\u001b[39mservice_args,\n\u001b[1;32m     72\u001b[0m     log_path\u001b[39m=\u001b[39mservice_log_path)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     RemoteWebDriver\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     77\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     78\u001b[0m         command_executor\u001b[39m=\u001b[39mChromeRemoteConnection(\n\u001b[1;32m     79\u001b[0m             remote_server_addr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mservice_url,\n\u001b[1;32m     80\u001b[0m             keep_alive\u001b[39m=\u001b[39mkeep_alive),\n\u001b[1;32m     81\u001b[0m         desired_capabilities\u001b[39m=\u001b[39mdesired_capabilities)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/inbox/lib/python3.9/site-packages/selenium/webdriver/common/service.py:98\u001b[0m, in \u001b[0;36mService.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_process_still_running()\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connectable():\n\u001b[1;32m    100\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/inbox/lib/python3.9/site-packages/selenium/webdriver/common/service.py:109\u001b[0m, in \u001b[0;36mService.assert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m return_code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess\u001b[39m.\u001b[39mpoll()\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mraise\u001b[39;00m WebDriverException(\n\u001b[1;32m    110\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mService \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m unexpectedly exited. Status code was: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    111\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath, return_code)\n\u001b[1;32m    112\u001b[0m     )\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /Users/jowonhyeong/Desktop/workspace/data/chromedriver unexpectedly exited. Status code was: -9\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "options.add_argument(\"lang=ko_KR\") # 한국어!\n",
    "driver = webdriver.Chrome(dir / 'chromedriver', options=options)\n",
    "data = {}\n",
    "\n",
    "def get_data(row):\n",
    "    acc, url = row\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    data[acc] = soup\n",
    "    print(acc, 'done')\n",
    "\n",
    "# with Pool(12) as p:\n",
    "#     p.apply(get_data, url_list)\n",
    "\n",
    "for i in url_list:\n",
    "    get_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_backup = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected at least 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mg:\\내 드라이브\\workspace\\jpynb\\edgar\\crawling.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m dic \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mACCESSION NUMBER\u001b[39m\u001b[39m'\u001b[39m: k}\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m dic\u001b[39m.\u001b[39mupdate(get_form_data(v))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m dic[\u001b[39m'\u001b[39m\u001b[39mFILER\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m get_filer_data(v)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m dic[\u001b[39m'\u001b[39m\u001b[39mCONFORMED SUBMISSION TYPE\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dic[\u001b[39m'\u001b[39m\u001b[39mFILER\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mFILING VALUES\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mFORM TYPE\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mappend(dic, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mg:\\내 드라이브\\workspace\\jpynb\\edgar\\crawling.ipynb 셀 6\u001b[0m in \u001b[0;36mget_filer_data\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_filer_data\u001b[39m(soup: BeautifulSoup) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     filer \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mfilerDiv\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     filer_data \u001b[39m=\u001b[39m [_parsing_filer(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m filer]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m filer_data\n",
      "\u001b[1;32mg:\\내 드라이브\\workspace\\jpynb\\edgar\\crawling.ipynb 셀 6\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_filer_data\u001b[39m(soup: BeautifulSoup) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     filer \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mfilerDiv\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     filer_data \u001b[39m=\u001b[39m [_parsing_filer(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m filer]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m filer_data\n",
      "\u001b[1;32mg:\\내 드라이브\\workspace\\jpynb\\edgar\\crawling.ipynb 셀 6\u001b[0m in \u001b[0;36m_parsing_filer\u001b[1;34m(filer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m coname \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m cik \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m addr \u001b[39m=\u001b[39m _parsing_addr(filer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# find div that inner text contains 'Business Address'\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m info \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: re\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m[^A-Za-z0-9\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.]+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, x\u001b[39m.\u001b[39mtext) , ident\u001b[39m.\u001b[39mcontents)\n",
      "\u001b[1;32mg:\\내 드라이브\\workspace\\jpynb\\edgar\\crawling.ipynb 셀 6\u001b[0m in \u001b[0;36m_parsing_addr\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m addr \u001b[39m=\u001b[39m address[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dic[\u001b[39m'\u001b[39m\u001b[39mSTREET 1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(addr)\u001b[39m.\u001b[39mtext\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m city, state, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(addr)\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m zip_num \u001b[39m=\u001b[39m _[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/%EB%82%B4%20%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B8%8C/workspace/jpynb/edgar/crawling.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m dic[\u001b[39m'\u001b[39m\u001b[39mCITY\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m city\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected at least 2, got 1)"
     ]
    }
   ],
   "source": [
    "# parsing data from sec.gov/Archives/edgar/data/.../...-index.html\n",
    "# data of SEC HEADER\n",
    "\n",
    "def _parsing_addr(soup: BeautifulSoup) -> dict:\n",
    "    dic = {}\n",
    "    address = soup.find_all('div', 'mailer')\n",
    "    addr = address[1].find_all('span').__iter__()\n",
    "    dic['STREET 1'] = next(addr).text\n",
    "    city, state, *_ = next(addr).text.strip().split(' ')\n",
    "    zip_num = _[0] if len(_) else None\n",
    "    dic['CITY'] = city\n",
    "    dic['STATE'] = state\n",
    "    dic['ZIP'] = zip_num\n",
    "    try: \n",
    "      phone_No = next(addr).text\n",
    "    except StopIteration:\n",
    "      phone_No = None\n",
    "    dic['BUSINESS PHONE'] = phone_No\n",
    "    return dic\n",
    "\n",
    "def _parsing_filer(filer: BeautifulSoup) -> dict:\n",
    "    ident = filer.find('p', {'class': 'identInfo'})\n",
    "    name = filer.find('span', {'class': 'companyName'}).text\n",
    "    coname = name.split('(')[0].strip()\n",
    "    cik = name.split('(')[1].split(')')[1].split(':')[1].strip()\n",
    "    addr = _parsing_addr(filer)\n",
    "    # find div that inner text contains 'Business Address'\n",
    "    info = map(lambda x: re.sub('[^A-Za-z0-9\\-\\s\\/\\.]+', '', x.text) , ident.contents)\n",
    "    info = [x.strip() for x in info if 0<len(x.strip())<25]\n",
    "    filer_data = {k: v for k, v in zip(info[::2], info[1::2])}\n",
    "    company_data = {}\n",
    "    company_data['COMPANY CONFORMED NAME'] = coname\n",
    "    company_data['CENTRAL INDEX KEY'] = cik\n",
    "    company_data['STANDARD INDUSTRIAL CLASSIFICATION'] = filer_data.get('SIC')\n",
    "    company_data['IRS NUMBER'] = filer_data.get('IRS No.')\n",
    "    company_data['STATE OF INCORPORATION'] = filer_data.get('State of Incorp.')\n",
    "    company_data['FISCAL YEAR END'] = filer_data.get('Fiscal Year End')\n",
    "    filing_values = {}\n",
    "    filing_values['FORM TYPE'] = filer_data.get('Type')\n",
    "    filing_values['SEC ACT'] = filer_data.get('Act')\n",
    "    filing_values['SEC FILE NUMBER'] = filer_data.get('File No.')\n",
    "    filing_values['FILM NUMBER'] = filer_data.get('Film No.')\n",
    "\n",
    "    filing = {}\n",
    "    filing['COMPANY DATA'] = company_data\n",
    "    filing['FILING VALUES'] = filing_values\n",
    "    filing['ADDRESS'] = addr\n",
    "    filing['FORMER COMPANY'] = {'FORMER CONFORMED NAME' : None, 'DATE OF NAME CHANGE': None}\n",
    "    return filing\n",
    "\n",
    "def get_form_data(soup):\n",
    "    dic = {}\n",
    "    div = v.find('div', {'id': 'formDiv'})\n",
    "    # find what that classname is 'formGrouping'\n",
    "    infohead = div.find_all('div', {'class': 'infoHead'})\n",
    "    info = div.find_all('div', {'class': 'info'})\n",
    "    for head, body in zip(infohead, info):\n",
    "        dic[head.text] = body.text\n",
    "    return dic\n",
    "    \n",
    "def get_filer_data(soup: BeautifulSoup) -> list:\n",
    "    filer = v.find_all('div', {'id': 'filerDiv'})\n",
    "    filer_data = [_parsing_filer(x) for x in filer]\n",
    "    return filer_data\n",
    "\n",
    "def df_to_text(df):\n",
    "  df = df[['ACCESSION NUMBER', 'CONFORMED SUBMISSION TYPE', 'Documents', 'Period of Report', 'Filing Date', 'FILER']]\n",
    "  df.columns = ['ACCESSION NUMBER', 'CONFORMED SUBMISSION TYPE', 'PUBLIC DOCUMENT COUNT', 'CONFORMED PERIOD OF REPORT', 'FILED AS OF DATE', 'FILER']\n",
    "  dic = {}\n",
    "  for row in range(len(df)):\n",
    "    text = ''.join([df['ACCESSION NUMBER'][row], '.hdr.sgml:'.ljust(15), df['FILED AS OF DATE'][row], '\\n'])\n",
    "    for i in df.columns:\n",
    "      if i != 'FILER':\n",
    "        col = ''.join([i, ':']).ljust(35)\n",
    "        text += f'{col}{df[i][row]}\\n'\n",
    "      else:\n",
    "        for j in df[i][row]:\n",
    "          text += '\\nFILER:\\n\\n'\n",
    "          for k, v in j.items():\n",
    "            text += f'\\t{k}:\\n'\n",
    "            for kk, vv in v.items():\n",
    "              kk = ''.join([kk, ':']).ljust(35)\n",
    "              text += f'\\t\\t{kk}{vv}\\n'\n",
    "    dic[df['ACCESSION NUMBER'][row]] = text\n",
    "  return dic\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  df = pd.DataFrame()\n",
    "  for k, v in data.items():\n",
    "      v: BeautifulSoup\n",
    "      dic = {'ACCESSION NUMBER': k}\n",
    "      dic.update(get_form_data(v))\n",
    "      dic['FILER'] = get_filer_data(v)\n",
    "      dic['CONFORMED SUBMISSION TYPE'] = dic['FILER'][0]['FILING VALUES']['FORM TYPE']\n",
    "      df = df.append(dic, ignore_index=True)\n",
    "      \n",
    "  text_dict = df_to_text(df)\n",
    "  \n",
    "\n",
    "# class is 'mailer'\n",
    "# find div that class is 'mialer'\n",
    "# find div that inner text contains 'Business Address'\n",
    "# find div that inner text contains 'Mailing Address'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMPANY CONFORMED NAME': 'DISNEY WALT CO',\n",
       " 'CENTRAL INDEX KEY': 'Filer)\\n CIK: 0000029082',\n",
       " 'STANDARD INDUSTRIAL CLASSIFICATION': '7990',\n",
       " 'IRS NUMBER': '950684440',\n",
       " 'STATE OF INCORPORATION': 'DE',\n",
       " 'FISCAL YEAR END': '0930'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filer = df['FILER'][0]\n",
    "filer[0]['COMPANY DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000029082-94-000015.hdr.sgml:     1994-06-30\n",
      "ACCESSION NUMBER:                  0000029082-94-000015\n",
      "CONFORMED SUBMISSION TYPE:         10-K/A\n",
      "PUBLIC DOCUMENT COUNT:             2\n",
      "CONFORMED PERIOD OF REPORT:        1993-09-30\n",
      "FILED AS OF DATE:                  1994-06-30\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\n",
      "\t\tCOMPANY CONFORMED NAME:            DISNEY WALT CO\n",
      "\t\tCENTRAL INDEX KEY:                 0000029082\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:7990\n",
      "\t\tIRS NUMBER:                        950684440\n",
      "\t\tSTATE OF INCORPORATION:            DE\n",
      "\t\tFISCAL YEAR END:                   0930\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:                         10-K/A\n",
      "\t\tSEC ACT:                           34\n",
      "\t\tSEC FILE NUMBER:                   001-04083\n",
      "\t\tFILM NUMBER:                       94537430\n",
      "\tADDRESS:\n",
      "\t\tSTREET 1:                          500 S BUENA VISTA ST\n",
      "\t\tCITY:                              BURBANK\n",
      "\t\tSTATE:                             CA\n",
      "\t\tZIP:                               91521\n",
      "\t\tBUSINESS PHONE:                    8185601000\n",
      "\tFORMER COMPANY:\n",
      "\t\tFORMER CONFORMED NAME:             None\n",
      "\t\tDATE OF NAME CHANGE:               None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ''.join([df['ACCESSION NUMBER'][0], '.hdr.sgml:'.ljust(15), df['FILED AS OF DATE'][0], '\\n'])\n",
    "for i in df.columns:\n",
    "  if i != 'FILER':\n",
    "    col = ''.join([i, ':']).ljust(35)\n",
    "    text += f'{col}{df[i][0]}\\n'\n",
    "  else:\n",
    "    for j in df[i][0]:\n",
    "      text += '\\nFILER:\\n\\n'\n",
    "      for k, v in j.items():\n",
    "        text += f'\\t{k}:\\n'\n",
    "        for kk, vv in v.items():\n",
    "          kk = ''.join([kk, ':']).ljust(35)\n",
    "          text += f'\\t\\t{kk}{vv}\\n'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILER:\n",
      "\n",
      "\t IRS No.                        950684440\n",
      "\t State of Incorp.               DE\n",
      "\t Fiscal Year End                0930\n",
      "\t Type                           10-K/A\n",
      "\t Act                            34\n",
      "\t File No.                       001-04083\n",
      "\t Film No.                       94537430\n",
      "\t SIC                            7990\n",
      "\t COMPANY CONFORMED NAME         DISNEY WALT CO\n",
      "\t street                         500 S BUENA VISTA ST\n",
      "\t city                           BURBANK\n",
      "\t state                          CA\n",
      "\t zip                            91521\n",
      "\t phone                          8185601000\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "  print('FILER:\\n')\n",
    "  for k, v in i.items():\n",
    "     print('\\t', k.ljust(30), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('CONFORMED SUBMISSION TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make simple web crawler\n",
    "def get_html(url):\n",
    "    # get html\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_soup(html):\n",
    "    # get soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "print(get_soup(get_html('https://www.sec.gov/Archives/edgar/data/933972/000093639296000235/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "TEST_URL = 'https://intoli.com/blog/making-chrome-headless-undetectable/chrome-headless-test.html'\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "options.add_argument(\"lang=ko_KR\") # 한국어!\n",
    "driver = webdriver.Chrome('chromedriver', chrome_options=options)\n",
    "\n",
    "driver.get(TEST_URL)\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'plugins', {get: function() {return[1, 2, 3, 4, 5]}})\")\n",
    "# lanuages 속성을 업데이트해주기\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'languages', {get: function() {return ['ko-KR', 'ko']}})\")\n",
    "\n",
    "user_agent = driver.find_element_by_css_selector('#user-agent').text\n",
    "plugins_length = driver.find_element_by_css_selector('#plugins-length').text\n",
    "languages = driver.find_element_by_css_selector('#languages').text\n",
    "\n",
    "print('User-Agent: ', user_agent)\n",
    "print('Plugin length: ', plugins_length)\n",
    "print('languages: ', languages)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "options.add_argument(\"lang=ko_KR\") # 한국어!\n",
    "driver = webdriver.Chrome('chromedriver', options=options)\n",
    "data = {}\n",
    "count: int = 0\n",
    "\n",
    "def get_url_list():\n",
    "  url_cast = 'https://sec.report/CIK/'\n",
    "  url_list = list(map(lambda x: ''.join([url_cast, x]), cik_list))\n",
    "  return url_list\n",
    "\n",
    "def find_some_tables(panels: list, text: str) -> Union[None, list]:\n",
    "  for i in panels:\n",
    "    if i.text.find(text) != -1:\n",
    "      trs = i.find_elements_by_tag_name('tr')\n",
    "      contents = [(x.find_element_by_xpath('td[1]').text, x.find_element_by_xpath('td[2]').text) for x in trs]\n",
    "      return contents\n",
    "  return None\n",
    "\n",
    "def find_and_get(url: str):\n",
    "  print(url)\n",
    "  driver.get(url)\n",
    "  print(count)\n",
    "  panels = driver.find_elements_by_class_name('panel')\n",
    "  details = find_some_tables(panels, 'Company Details')\n",
    "  relations = find_some_tables(panels, 'Related SEC Filings')\n",
    "  callback_func({'details': details, 'relations': relations})\n",
    "\n",
    "def callback_func(result):\n",
    "    data[str(cik_list[count])] = result\n",
    "    count = count + 1\n",
    "    print(count)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # pool = Pool(processes=12)\n",
    "    # pool.map(find_and_get, url_list[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5d32f057a3c8bd6a68a4140140c1c01731d179f143636ed2ae590c641a050cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
